#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ä¸“å®¶çŸ¥è¯†é¢„å¤„ç†ä¸åµŒå…¥ç”Ÿæˆè„šæœ¬
1. å¯¹ä¸“å®¶æ•°æ®è¿›è¡Œé¢„å¤„ç†
2. ä½¿ç”¨Qwenæ¨¡å‹ç”Ÿæˆè¯­ä¹‰åµŒå…¥
3. ç¼“å­˜åµŒå…¥å‘é‡åˆ°æœ¬åœ°æ–‡ä»¶
"""

import json
import os
import numpy as np
import torch
import re
from typing import Dict, List, Optional, Tuple
from transformers import AutoModel, AutoTokenizer
from datetime import datetime


class ExpertKnowledgeProcessor:
    """ä¸“å®¶çŸ¥è¯†é¢„å¤„ç†ä¸åµŒå…¥ç”Ÿæˆå™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        
        # åŠ è½½é…ç½®æ–‡ä»¶
        self._load_config_from_file()
        
        # åˆå¹¶é…ç½®ï¼Œå‘½ä»¤è¡Œé…ç½®ä¼˜å…ˆ
        self.full_config = {**self.file_config, **config}
        
        # é…ç½®å‚æ•°
        self.knowledge_base_path = self.full_config.get(
            'knowledge_base_path',
            '/Users/Williamhiler/Documents/my-project/train/v5/data/expert_knowledge/expert_knowledge_base.json'
        )
        
        self.embedding_cache_dir = self.full_config.get(
            'embedding_cache_dir',
            '/Users/Williamhiler/Documents/my-project/train/v5/data/expert_knowledge/embeddings'
        )
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–LLMé…ç½®
        self.llm_config = self.full_config.get('expert_analysis_llm', {})
        self.force_local = self.llm_config.get('force_local', True)
        self.model_name = self.llm_config.get('model_name', '/Users/Williamhiler/Documents/my-project/train/models/cache')  # ä¿®æ­£ä¸ºå®é™…çš„æœ¬åœ°æ¨¡å‹è·¯å¾„
        
        self.embedding_dim = self.full_config.get('embedding_dim', 512)
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(self.embedding_cache_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
        self.model, self.tokenizer = self._initialize_model()
        
        # åŠ è½½çŸ¥è¯†åº“
        self.knowledge_base = self._load_knowledge_base()
    
    
    def _load_config_from_file(self):
        """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
        import yaml
        
        self.file_config = {}
        
        # é…ç½®æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        config_files = [
            "/Users/Williamhiler/Documents/my-project/train/config/config.yaml",
            "/Users/Williamhiler/Documents/my-project/train/v5/configs/v5_config.yaml"
        ]
        
        for config_file in config_files:
            if os.path.exists(config_file):
                try:
                    with open(config_file, 'r', encoding='utf-8') as f:
                        file_config = yaml.safe_load(f)
                        self.file_config.update(file_config)
                        print(f"âœ… åŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
                except Exception as e:
                    print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥ {config_file}: {e}")
            else:
                print(f"âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
    
    
    def _initialize_model(self):
        """åˆå§‹åŒ–Qwenæ¨¡å‹å’Œåˆ†è¯å™¨"""
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        print(f"   æ¨¡å‹åç§°/è·¯å¾„: {self.model_name}")
        print(f"   å¼ºåˆ¶æœ¬åœ°æ¨¡å¼: {self.force_local}")
        
        try:
            # æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
            if torch.cuda.is_available():
                device = torch.device("cuda")
                print(f"   GPUå¯ç”¨ï¼Œå°†ä½¿ç”¨: {torch.cuda.get_device_name(0)}")
                dtype = torch.float16
            else:
                device = torch.device("cpu")
                print("   GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
                dtype = torch.float32  # CPUä¸Šä½¿ç”¨float32ä»¥æé«˜å…¼å®¹æ€§
            
            if self.force_local:
                print(f"ğŸ”’ å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œä¸å°è¯•ä¸‹è½½")
                
                # æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ˜¯å¦å­˜åœ¨
                if not os.path.exists(self.model_name):
                    raise FileNotFoundError(f"æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_name}")
                
                # å¼ºåˆ¶ä»æœ¬åœ°åŠ è½½ï¼Œä¸å°è¯•ä¸‹è½½
                tokenizer = AutoTokenizer.from_pretrained(
                    self.model_name,
                    local_files_only=True,  # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
                    trust_remote_code=True
                )
                model = AutoModel.from_pretrained(
                    self.model_name,
                    dtype=dtype,
                    device_map=device,  # å¼ºåˆ¶ä½¿ç”¨æŒ‡å®šè®¾å¤‡
                    local_files_only=True,  # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
                    trust_remote_code=True
                )
            else:
                # æ­£å¸¸æ¨¡å¼ï¼Œå…ˆå°è¯•æœ¬åœ°ï¼Œå†å°è¯•ä¸‹è½½
                print("æ­£å¸¸æ¨¡å¼ï¼Œå…è®¸ä¸‹è½½æ¨¡å‹")
                tokenizer = AutoTokenizer.from_pretrained(self.model_name)
                model = AutoModel.from_pretrained(
                    self.model_name,
                    dtype=dtype,
                    device_map=device
                )
            
            # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            model.to(device)
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {device}")
            return model, tokenizer
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    
    def _load_knowledge_base(self) -> Dict:
        """åŠ è½½ä¸“å®¶çŸ¥è¯†åº“"""
        print(f"æ­£åœ¨åŠ è½½çŸ¥è¯†åº“: {self.knowledge_base_path}")
        
        try:
            with open(self.knowledge_base_path, 'r', encoding='utf-8') as f:
                knowledge_base = json.load(f)
            
            print(f"âœ… çŸ¥è¯†åº“åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(knowledge_base['knowledge_units'])} ä¸ªçŸ¥è¯†å•å…ƒ")
            return knowledge_base
        except Exception as e:
            print(f"âŒ çŸ¥è¯†åº“åŠ è½½å¤±è´¥: {e}")
            raise
    
    
    def preprocess_knowledge_unit(self, unit: Dict) -> Dict:
        """é¢„å¤„ç†å•ä¸ªçŸ¥è¯†å•å…ƒ"""
        processed_unit = unit.copy()
        
        # 1. æ–‡æœ¬æ¸…æ´—
        content = unit.get('content', '')
        title = unit.get('title', '')
        
        # æ¸…æ´—å‡½æ•°
        def clean_text(text: str) -> str:
            if not text:
                return ""
            
            # å»é™¤ç‰¹æ®Šå­—ç¬¦å’Œå¤šä½™ç©ºæ ¼
            text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9.,ï¼Œã€‚ï¼›;:!?ï¼ï¼Ÿ\s\d.\d+]', ' ', text)
            
            # å»é™¤å¤šä½™ç©ºæ ¼
            text = re.sub(r'\s+', ' ', text).strip()
            
            # å»é™¤è¡Œé¦–è¡Œå°¾ç©ºæ ¼
            text = text.strip()
            
            return text
        
        # æ¸…æ´—æ ‡é¢˜å’Œå†…å®¹
        processed_unit['cleaned_title'] = clean_text(title)
        processed_unit['cleaned_content'] = clean_text(content)
        
        # 2. æ„å»ºå®Œæ•´æ–‡æœ¬è¡¨ç¤º
        text_parts = [
            processed_unit['cleaned_title'],
            processed_unit['cleaned_content'],
            f"çŸ¥è¯†ç±»å‹: {processed_unit.get('knowledge_type', '')}",
            f"å…³é”®æ¦‚å¿µ: {', '.join(processed_unit.get('key_concepts', []))}"
        ]
        
        # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        text_parts = [part for part in text_parts if part.strip()]
        
        # åˆå¹¶ä¸ºå®Œæ•´æ–‡æœ¬
        processed_unit['full_text'] = " ".join(text_parts)
        
        return processed_unit
    
    
    def preprocess_all_knowledge(self) -> List[Dict]:
        """é¢„å¤„ç†æ‰€æœ‰çŸ¥è¯†å•å…ƒ"""
        print("å¼€å§‹é¢„å¤„ç†æ‰€æœ‰çŸ¥è¯†å•å…ƒ...")
        
        processed_units = []
        for i, unit in enumerate(self.knowledge_base['knowledge_units']):
            processed_unit = self.preprocess_knowledge_unit(unit)
            processed_units.append(processed_unit)
            
            if (i + 1) % 100 == 0:
                print(f"å·²é¢„å¤„ç† {i + 1}/{len(self.knowledge_base['knowledge_units'])} ä¸ªçŸ¥è¯†å•å…ƒ")
        
        print(f"âœ… é¢„å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(processed_units)} ä¸ªçŸ¥è¯†å•å…ƒ")
        return processed_units
    
    
    def generate_embedding(self, text: str) -> np.ndarray:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥"""
        try:
            # åˆ†è¯
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            
            # ç§»è‡³æ¨¡å‹è®¾å¤‡
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
            
            # ç”ŸæˆåµŒå…¥
            with torch.no_grad():
                outputs = self.model(**inputs)
                # ä½¿ç”¨æœ€åä¸€å±‚çš„CLS tokenä½œä¸ºåµŒå…¥
                embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()[0]
            
            # å½’ä¸€åŒ–
            embedding = embedding / np.linalg.norm(embedding)
            return embedding
        except Exception as e:
            print(f"âŒ ç”ŸæˆåµŒå…¥å¤±è´¥: {e}")
            # è¿”å›å…¨é›¶å‘é‡ä½œä¸º fallback
            return np.zeros(self.embedding_dim)
    
    
    def generate_all_embeddings(self, processed_units: List[Dict]) -> np.ndarray:
        """ç”Ÿæˆæ‰€æœ‰çŸ¥è¯†å•å…ƒçš„åµŒå…¥"""
        import time
        
        print("å¼€å§‹ç”ŸæˆåµŒå…¥å‘é‡...")
        
        # æå–æ‰€æœ‰æ–‡æœ¬
        texts = [unit['full_text'] for unit in processed_units]
        batch_size = 4  # è¿›ä¸€æ­¥å‡å°‘æ‰¹å¤„ç†å¤§å°ï¼Œæé«˜å¤„ç†é€Ÿåº¦
        total_texts = len(texts)
        embeddings = []
        
        print(f"æ€»æ–‡æœ¬æ•°: {total_texts}, æ‰¹å¤„ç†å¤§å°: {batch_size}, æ€»æ‰¹æ¬¡æ•°: {((total_texts - 1) // batch_size) + 1}")
        print(f"æ¨¡å‹è®¾å¤‡: {self.model.device}")
        
        start_time = time.time()
        
        # ä¸´æ—¶ä¿å­˜æ–‡ä»¶è·¯å¾„
        partial_embedding_file = os.path.join(self.embedding_cache_dir, 'partial_knowledge_embeddings.npy')
        partial_config_file = os.path.join(self.embedding_cache_dir, 'partial_embedding_config.json')
        
        # åŠ¨æ€è·å–åµŒå…¥ç»´åº¦çš„æ ‡å¿—
        embedding_dim_set = False
        
        for i in range(0, total_texts, batch_size):
            # è·å–å½“å‰æ‰¹æ¬¡
            batch_texts = texts[i:i+batch_size]
            batch_num = i // batch_size + 1
            current_batch_size = len(batch_texts)
            
            print(f"\nå¤„ç†æ‰¹æ¬¡ {batch_num}/{((total_texts - 1) // batch_size) + 1}: å¤„ç† {i+1}-{min(i+batch_size, total_texts)}/{total_texts} ä¸ªæ–‡æœ¬")
            
            batch_start_time = time.time()
            
            try:
                # æ­¥éª¤1: æ‰¹é‡åˆ†è¯
                tokenize_start = time.time()
                inputs = self.tokenizer(
                    batch_texts,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=512
                )
                tokenize_time = time.time() - tokenize_start
                print(f"  åˆ†è¯å®Œæˆ: {tokenize_time:.2f}ç§’")
                
                # æ­¥éª¤2: ç§»è‡³æ¨¡å‹è®¾å¤‡
                to_device_start = time.time()
                inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
                to_device_time = time.time() - to_device_start
                print(f"  ç§»è‡³è®¾å¤‡å®Œæˆ: {to_device_time:.2f}ç§’")
                
                # æ­¥éª¤3: æ‰¹é‡ç”ŸæˆåµŒå…¥
                generate_start = time.time()
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    # ä½¿ç”¨æœ€åä¸€å±‚çš„CLS tokenä½œä¸ºåµŒå…¥
                    batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                generate_time = time.time() - generate_start
                print(f"  åµŒå…¥ç”Ÿæˆå®Œæˆ: {generate_time:.2f}ç§’")
                
                # æ­¥éª¤3.5: åŠ¨æ€è®¾ç½®åµŒå…¥ç»´åº¦ï¼ˆä»…ç¬¬ä¸€æ¬¡ï¼‰
                if not embedding_dim_set:
                    actual_embedding_dim = batch_embeddings.shape[1]
                    if actual_embedding_dim != self.embedding_dim:
                        print(f"  æ³¨æ„: å®é™…åµŒå…¥ç»´åº¦ä¸é…ç½®ç»´åº¦ä¸ä¸€è‡´")
                        print(f"  é…ç½®ç»´åº¦: {self.embedding_dim}, å®é™…ç»´åº¦: {actual_embedding_dim}")
                        print(f"  åŠ¨æ€æ›´æ–°åµŒå…¥ç»´åº¦ä¸º: {actual_embedding_dim}")
                        self.embedding_dim = actual_embedding_dim
                    embedding_dim_set = True
                
                # æ­¥éª¤4: æ‰¹é‡å½’ä¸€åŒ–
                normalize_start = time.time()
                batch_embeddings = batch_embeddings / np.linalg.norm(batch_embeddings, axis=1, keepdims=True)
                normalize_time = time.time() - normalize_start
                print(f"  å½’ä¸€åŒ–å®Œæˆ: {normalize_time:.2f}ç§’")
                
                # æ­¥éª¤5: æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                embeddings.extend(batch_embeddings.tolist())
                
            except Exception as e:
                print(f"âŒ æ‰¹é‡ç”ŸæˆåµŒå…¥å¤±è´¥ï¼Œæ‰¹æ¬¡ {batch_num}: {e}")
                # å¯¹å¤±è´¥çš„æ‰¹æ¬¡è¿›è¡Œå•æ¡å¤„ç†
                for j, text in enumerate(batch_texts):
                    embedding = self.generate_embedding(text)
                    embeddings.append(embedding.tolist())
            
            batch_end_time = time.time()
            batch_time = batch_end_time - batch_start_time
            
            # æ‰“å°å½“å‰æ‰¹æ¬¡ç»Ÿè®¡
            print(f"  æ‰¹æ¬¡ {batch_num} å¤„ç†å®Œæˆï¼Œè€—æ—¶: {batch_time:.2f}ç§’")
            print(f"  æ‰¹æ¬¡é€Ÿåº¦: {current_batch_size/batch_time:.2f}æ–‡æœ¬/ç§’")
            
            # æ‰“å°æ•´ä½“è¿›åº¦å’Œé¢„è®¡å‰©ä½™æ—¶é—´
            processed = min(i + batch_size, total_texts)
            elapsed_time = time.time() - start_time
            texts_per_second = processed / elapsed_time if elapsed_time > 0 else 0
            remaining_time = (total_texts - processed) / texts_per_second if texts_per_second > 0 else 0
            
            print(f"  ç´¯è®¡è¿›åº¦: {processed}/{total_texts} ä¸ªåµŒå…¥å‘é‡")
            print(f"  ç´¯è®¡å¹³å‡é€Ÿåº¦: {texts_per_second:.2f}æ–‡æœ¬/ç§’")
            print(f"  é¢„è®¡å‰©ä½™æ—¶é—´: {remaining_time:.2f}ç§’ ({remaining_time/60:.1f}åˆ†é’Ÿ)")
            
            # å®šæœŸä¿å­˜è¿›åº¦ï¼ˆæ¯å¤„ç†10ä¸ªæ‰¹æ¬¡æˆ–æœ€åä¸€ä¸ªæ‰¹æ¬¡ï¼‰
            if batch_num % 10 == 0 or processed == total_texts:
                print(f"\nğŸ’¾ å®šæœŸä¿å­˜è¿›åº¦...")
                
                # ä¿å­˜å½“å‰å·²ç”Ÿæˆçš„åµŒå…¥
                np.save(partial_embedding_file, np.array(embeddings))
                
                # ä¿å­˜é…ç½®ä¿¡æ¯
                partial_config = {
                    'model_name': self.model_name,
                    'embedding_dim': self.embedding_dim,
                    'generated_timestamp': datetime.now().isoformat(),
                    'processed_count': processed,
                    'total_count': total_texts,
                    'last_batch_num': batch_num
                }
                
                with open(partial_config_file, 'w', encoding='utf-8') as f:
                    json.dump(partial_config, f, ensure_ascii=False, indent=2)
                
                print(f"âœ… è¿›åº¦å·²ä¿å­˜: {processed}/{total_texts} ä¸ªåµŒå…¥å‘é‡")
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        embeddings_array = np.array(embeddings)
        print(f"\nâœ… åµŒå…¥ç”Ÿæˆå®Œæˆï¼ŒåµŒå…¥å½¢çŠ¶: {embeddings_array.shape}")
        print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
        print(f"æ€»å¹³å‡é€Ÿåº¦: {total_texts/total_time:.2f}æ–‡æœ¬/ç§’")
        
        return embeddings_array
    
    
    def save_embeddings(self, embeddings: np.ndarray, processed_units: List[Dict]):
        """ä¿å­˜åµŒå…¥å‘é‡å’Œç›¸å…³é…ç½®"""
        print("å¼€å§‹ä¿å­˜åµŒå…¥å‘é‡...")
        
        # ä¿å­˜åµŒå…¥å‘é‡
        embedding_file = os.path.join(self.embedding_cache_dir, 'knowledge_embeddings.npy')
        np.save(embedding_file, embeddings)
        print(f"âœ… åµŒå…¥å‘é‡å·²ä¿å­˜åˆ°: {embedding_file}")
        
        # ä¿å­˜åµŒå…¥é…ç½®
        embedding_config = {
            'model_name': self.model_name,
            'embedding_dim': self.embedding_dim,
            'generated_timestamp': datetime.now().isoformat(),
            'total_knowledge_units': len(processed_units),
            'preprocessing_steps': [
                'æ–‡æœ¬æ¸…æ´—',
                'æ ‡é¢˜å†…å®¹åˆå¹¶',
                'çŸ¥è¯†ç±»å‹æ·»åŠ ',
                'å…³é”®æ¦‚å¿µæ•´åˆ'
            ],
            'knowledge_ids': [unit.get('id', str(i)) for i, unit in enumerate(processed_units)]
        }
        
        config_file = os.path.join(self.embedding_cache_dir, 'embedding_config.json')
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(embedding_config, f, ensure_ascii=False, indent=2)
        print(f"âœ… åµŒå…¥é…ç½®å·²ä¿å­˜åˆ°: {config_file}")
        
        # ä¿å­˜å¤„ç†åçš„çŸ¥è¯†å•å…ƒï¼ˆå¯é€‰ï¼‰
        processed_file = os.path.join(self.embedding_cache_dir, 'processed_knowledge_units.json')
        with open(processed_file, 'w', encoding='utf-8') as f:
            json.dump(processed_units, f, ensure_ascii=False, indent=2)
        print(f"âœ… å¤„ç†åçš„çŸ¥è¯†å•å…ƒå·²ä¿å­˜åˆ°: {processed_file}")
    
    
    def load_embeddings(self) -> Tuple[np.ndarray, Dict]:
        """åŠ è½½å·²ä¿å­˜çš„åµŒå…¥å‘é‡"""
        print("åŠ è½½å·²ä¿å­˜çš„åµŒå…¥å‘é‡...")
        
        # åŠ è½½åµŒå…¥å‘é‡
        embedding_file = os.path.join(self.embedding_cache_dir, 'knowledge_embeddings.npy')
        if not os.path.exists(embedding_file):
            raise FileNotFoundError(f"åµŒå…¥å‘é‡æ–‡ä»¶ä¸å­˜åœ¨: {embedding_file}")
        
        embeddings = np.load(embedding_file)
        print(f"âœ… åŠ è½½åµŒå…¥å‘é‡æˆåŠŸï¼Œå½¢çŠ¶: {embeddings.shape}")
        
        # åŠ è½½åµŒå…¥é…ç½®
        config_file = os.path.join(self.embedding_cache_dir, 'embedding_config.json')
        if not os.path.exists(config_file):
            raise FileNotFoundError(f"åµŒå…¥é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        
        with open(config_file, 'r', encoding='utf-8') as f:
            embedding_config = json.load(f)
        print(f"âœ… åŠ è½½åµŒå…¥é…ç½®æˆåŠŸ")
        
        return embeddings, embedding_config
    
    
    def run_full_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„é¢„å¤„ç†å’ŒåµŒå…¥ç”Ÿæˆæµç¨‹"""
        print("=== ä¸“å®¶çŸ¥è¯†é¢„å¤„ç†ä¸åµŒå…¥ç”Ÿæˆæµç¨‹ ===")
        
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰éƒ¨åˆ†ç”Ÿæˆçš„åµŒå…¥
            partial_embedding_file = os.path.join(self.embedding_cache_dir, 'partial_knowledge_embeddings.npy')
            partial_config_file = os.path.join(self.embedding_cache_dir, 'partial_embedding_config.json')
            
            start_idx = 0
            existing_embeddings = []
            
            # 1. é¢„å¤„ç†æ‰€æœ‰çŸ¥è¯†å•å…ƒ
            processed_units = self.preprocess_all_knowledge()
            total_units = len(processed_units)
            
            # 2. æ£€æŸ¥æ–­ç‚¹
            if os.path.exists(partial_embedding_file) and os.path.exists(partial_config_file):
                print("\nğŸ” å‘ç°éƒ¨åˆ†ç”Ÿæˆçš„åµŒå…¥ï¼Œå°è¯•ä»æ–­ç‚¹ç»§ç»­...")
                
                # åŠ è½½éƒ¨åˆ†åµŒå…¥
                existing_embeddings = np.load(partial_embedding_file).tolist()
                with open(partial_config_file, 'r', encoding='utf-8') as f:
                    partial_config = json.load(f)
                
                start_idx = len(existing_embeddings)
                print(f"ğŸ“‹ å·²ç”Ÿæˆ {start_idx}/{total_units} ä¸ªåµŒå…¥å‘é‡ï¼Œå°†ä»ç¬¬ {start_idx + 1} ä¸ªå¼€å§‹ç»§ç»­")
                
                # æ£€æŸ¥ä¸€è‡´æ€§
                if start_idx > total_units:
                    print("âš ï¸  éƒ¨åˆ†åµŒå…¥æ•°é‡è¶…è¿‡æ€»çŸ¥è¯†å•å…ƒæ•°ï¼Œå°†é‡æ–°ç”Ÿæˆ")
                    start_idx = 0
                    existing_embeddings = []
            
            # 3. ç”ŸæˆåµŒå…¥å‘é‡
            if start_idx == 0:
                # ä»å¤´å¼€å§‹ç”Ÿæˆ
                embeddings = self.generate_all_embeddings(processed_units)
            else:
                # ä»æ–­ç‚¹ç»§ç»­ç”Ÿæˆ
                remaining_units = processed_units[start_idx:]
                print(f"\nå¼€å§‹ç»§ç»­ç”ŸæˆåµŒå…¥å‘é‡...")
                print(f"æ€»æ–‡æœ¬æ•°: {total_units}, å·²ç”Ÿæˆ: {start_idx}, å‰©ä½™: {len(remaining_units)}")
                
                remaining_embeddings = self.generate_all_embeddings(remaining_units)
                embeddings = np.array(existing_embeddings + remaining_embeddings.tolist())
            
            # 4. ä¿å­˜å®Œæ•´åµŒå…¥
            self.save_embeddings(embeddings, processed_units)
            
            # 5. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(partial_embedding_file):
                os.remove(partial_embedding_file)
            if os.path.exists(partial_config_file):
                os.remove(partial_config_file)
            print(f"âœ… å·²æ¸…ç†ä¸´æ—¶æ–­ç‚¹æ–‡ä»¶")
            
            print("\nğŸ‰ å®Œæ•´æµç¨‹æ‰§è¡ŒæˆåŠŸï¼")
            print(f"- é¢„å¤„ç†çŸ¥è¯†å•å…ƒ: {len(processed_units)} ä¸ª")
            print(f"- ç”ŸæˆåµŒå…¥å‘é‡: {embeddings.shape}")
            print(f"- åµŒå…¥ç¼“å­˜ç›®å½•: {self.embedding_cache_dir}")
            
        except Exception as e:
            print(f"\nâŒ æµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    
    def validate_embeddings(self):
        """éªŒè¯åµŒå…¥å‘é‡çš„è´¨é‡"""
        print("å¼€å§‹éªŒè¯åµŒå…¥å‘é‡è´¨é‡...")
        
        try:
            # åŠ è½½åµŒå…¥
            embeddings, config = self.load_embeddings()
            
            # 1. æ£€æŸ¥åµŒå…¥ç»´åº¦
            actual_dim = embeddings.shape[1]
            print(f"åµŒå…¥ç»´åº¦: {actual_dim}")
            
            # ä½¿ç”¨å®é™…åµŒå…¥ç»´åº¦ï¼Œè€Œä¸æ˜¯ä¾èµ–é…ç½®å€¼
            print(f"é…ç½®ç»´åº¦: {self.embedding_dim}")
            
            # 2. æ£€æŸ¥åµŒå…¥æ•°é‡
            print(f"åµŒå…¥æ•°é‡: {embeddings.shape[0]}")
            assert embeddings.shape[0] == len(self.knowledge_base['knowledge_units']), \
                f"åµŒå…¥æ•°é‡ä¸çŸ¥è¯†å•å…ƒæ•°é‡ä¸åŒ¹é…: {embeddings.shape[0]} != {len(self.knowledge_base['knowledge_units'])}"
            
            # 3. æ£€æŸ¥å½’ä¸€åŒ–
            norms = np.linalg.norm(embeddings, axis=1)
            avg_norm = np.mean(norms)
            print(f"åµŒå…¥å‘é‡å¹³å‡èŒƒæ•°: {avg_norm:.6f}")
            assert abs(avg_norm - 1.0) < 0.01, f"åµŒå…¥å‘é‡æœªæ­£ç¡®å½’ä¸€åŒ–ï¼Œå¹³å‡èŒƒæ•°: {avg_norm}"
            
            # 4. æ£€æŸ¥åµŒå…¥å¤šæ ·æ€§ï¼ˆè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µçš„å¹³å‡å€¼ï¼‰
            sample_size = min(100, len(embeddings))
            sample_embeddings = embeddings[:sample_size]
            similarity_matrix = np.dot(sample_embeddings, sample_embeddings.T)
            
            # æ’é™¤å¯¹è§’çº¿å…ƒç´ ï¼ˆè‡ªèº«ç›¸ä¼¼åº¦ï¼‰
            mask = np.eye(similarity_matrix.shape[0], dtype=bool)
            avg_similarity = np.mean(similarity_matrix[~mask])
            print(f"åµŒå…¥å‘é‡å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.6f}")
            
            print("\nâœ… åµŒå…¥å‘é‡éªŒè¯é€šè¿‡ï¼")
            print("åµŒå…¥å‘é‡è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥ç”¨äºåŒ¹é…ä»»åŠ¡ã€‚")
            
        except Exception as e:
            print(f"\nâŒ åµŒå…¥å‘é‡éªŒè¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®
    config = {
        'knowledge_base_path': '/Users/Williamhiler/Documents/my-project/train/v5/data/expert_knowledge/expert_knowledge_base.json',
        'embedding_cache_dir': '/Users/Williamhiler/Documents/my-project/train/v5/data/expert_knowledge/embeddings',
        'model_name': 'Qwen/Qwen2-0.5B',
        'embedding_dim': 512
    }
    
    # åˆ›å»ºå¤„ç†å™¨å®ä¾‹
    processor = ExpertKnowledgeProcessor(config)
    
    # è¿è¡Œå®Œæ•´æµç¨‹
    processor.run_full_pipeline()
    
    # éªŒè¯åµŒå…¥è´¨é‡
    processor.validate_embeddings()


if __name__ == "__main__":
    main()