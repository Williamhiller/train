#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
åŸºäºQwenå¤§æ¨¡å‹çš„æ™ºèƒ½ä¸“å®¶çŸ¥è¯†åŒ¹é…å™¨
åˆ©ç”¨è¯­ä¹‰ç†è§£èƒ½åŠ›å®ç°ç²¾å‡†çš„çŸ¥è¯†åŒ¹é…
"""

import sys
import os
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))

import json
import numpy as np
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import torch
from transformers import AutoModel, AutoTokenizer
from sklearn.metrics.pairwise import cosine_similarity

# å¯¼å…¥æœ€æ–°çš„ä¸Šä¸‹æ–‡ç”Ÿæˆå™¨
from v5.utils.data_processing.context_generator import ContextGenerator


class QwenKnowledgeMatcher:
    """åŸºäºQwençš„æ™ºèƒ½çŸ¥è¯†åŒ¹é…å™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.knowledge_base = None
        self.embeddings = None
        self.knowledge_units = []
        
        # åŠ è½½é…ç½®
        self.llm_config = config.get('expert_analysis_llm', {})
        self.force_local = self.llm_config.get('force_local', True)  # é»˜è®¤ä¸ºTrueï¼Œå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ¨¡å‹
        self.local_model_path = self.llm_config.get('model_name', config.get('qwen', {}).get('model_path', '/Users/Williamhiler/Documents/my-project/train/models/cache'))  # ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹è·¯å¾„
        
        # åŠ è½½çŸ¥è¯†åº“
        self.load_knowledge_base()
        
        # åˆå§‹åŒ–ä¸Šä¸‹æ–‡ç”Ÿæˆå™¨
        self.context_generator = ContextGenerator()
        
        # åˆå§‹åŒ–Qwenæ¨¡å‹å’Œåˆ†è¯å™¨
        self.model, self.tokenizer = self._initialize_qwen_model()
        
        # ç”ŸæˆçŸ¥è¯†åº“åµŒå…¥
        self.generate_knowledge_embeddings()
    
    
    def _initialize_qwen_model(self):
        """åˆå§‹åŒ–Qwenæ¨¡å‹å’Œåˆ†è¯å™¨"""
        print("æ­£åœ¨åŠ è½½Qwenæ¨¡å‹...")
        
        try:
            if self.force_local:
                print(f"ğŸ”’ å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ¨¡å‹")
                print(f"   æœ¬åœ°æ¨¡å‹è·¯å¾„: {self.local_model_path}")
                
                # æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ˜¯å¦å­˜åœ¨
                if not os.path.exists(self.local_model_path):
                    raise FileNotFoundError(f"æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.local_model_path}")
                
                # å¼ºåˆ¶ä»æœ¬åœ°åŠ è½½ï¼Œä¸å°è¯•ä¸‹è½½
                tokenizer = AutoTokenizer.from_pretrained(
                    self.local_model_path,
                    local_files_only=True  # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
                )
                model = AutoModel.from_pretrained(
                    self.local_model_path,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    local_files_only=True  # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
                )
            else:
                # æ­£å¸¸æ¨¡å¼ï¼Œå…ˆå°è¯•æœ¬åœ°ï¼Œå†å°è¯•ä¸‹è½½
                local_model_path = "/Users/Williamhiler/Documents/my-project/train/v5/models/cache"
                
                if os.path.exists(local_model_path):
                    print(f"ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {local_model_path}")
                    tokenizer = AutoTokenizer.from_pretrained(local_model_path)
                    model = AutoModel.from_pretrained(
                        local_model_path,
                        torch_dtype=torch.float16,
                        device_map="auto"
                    )
                else:
                    print(f"æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨æ¨¡å‹åç§°...")
                    model_name = "Qwen/Qwen2-0.5B"
                    tokenizer = AutoTokenizer.from_pretrained(model_name)
                    model = AutoModel.from_pretrained(
                        model_name,
                        torch_dtype=torch.float16,
                        device_map="auto"
                    )
            
            print("âœ… Qwenæ¨¡å‹åŠ è½½æˆåŠŸ")
            return model, tokenizer
        except Exception as e:
            print(f"âŒ åŠ è½½Qwenæ¨¡å‹å¤±è´¥: {e}")
            print(f"   æœ¬åœ°æ¨¡å‹è·¯å¾„: {self.local_model_path}")
            print(f"   å¼ºåˆ¶æœ¬åœ°æ¨¡å¼: {self.force_local}")
            raise
    
    
    def load_knowledge_base(self):
        """åŠ è½½çŸ¥è¯†åº“"""
        knowledge_base_path = "/Users/Williamhiler/Documents/my-project/train/v5/data/expert_knowledge/expert_knowledge_base.json"
        
        try:
            with open(knowledge_base_path, 'r', encoding='utf-8') as f:
                self.knowledge_base = json.load(f)
            
            self.knowledge_units = self.knowledge_base["knowledge_units"]
            print(f"âœ… æˆåŠŸåŠ è½½çŸ¥è¯†åº“: {len(self.knowledge_units)} ä¸ªçŸ¥è¯†å•å…ƒ")
        except Exception as e:
            print(f"âŒ åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}")
            self.knowledge_base = None
    
    
    def generate_embedding(self, text: str) -> np.ndarray:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥"""
        try:
            # åˆ†è¯
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            
            # ç§»è‡³GPU
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
            
            # ç”ŸæˆåµŒå…¥
            with torch.no_grad():
                outputs = self.model(**inputs)
                # ä½¿ç”¨æœ€åä¸€å±‚çš„CLS tokenä½œä¸ºåµŒå…¥
                embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()[0]
            
            # å½’ä¸€åŒ–
            embedding = embedding / np.linalg.norm(embedding)
            return embedding
        except Exception as e:
            print(f"âŒ ç”ŸæˆåµŒå…¥å¤±è´¥: {e}")
            # è¿”å›å…¨é›¶å‘é‡ä½œä¸º fallback
            return np.zeros(512)
    
    
    def generate_knowledge_embeddings(self):
        """ç”ŸæˆçŸ¥è¯†åº“ä¸­æ‰€æœ‰çŸ¥è¯†å•å…ƒçš„åµŒå…¥"""
        # å…ˆå°è¯•åŠ è½½é¢„ç”Ÿæˆçš„åµŒå…¥
        if self._load_precomputed_embeddings():
            return
        
        print("æ­£åœ¨ç”ŸæˆçŸ¥è¯†åº“åµŒå…¥...")
        
        if not self.knowledge_units:
            print("âŒ çŸ¥è¯†åº“ä¸ºç©ºï¼Œæ— æ³•ç”ŸæˆåµŒå…¥")
            return
        
        try:
            # ç”Ÿæˆæ‰€æœ‰çŸ¥è¯†å•å…ƒçš„åµŒå…¥
            self.embeddings = []
            for i, unit in enumerate(self.knowledge_units):
                # æ„å»ºçŸ¥è¯†å•å…ƒçš„æ–‡æœ¬è¡¨ç¤º
                knowledge_text = self._construct_knowledge_text(unit)
                # ç”ŸæˆåµŒå…¥
                embedding = self.generate_embedding(knowledge_text)
                self.embeddings.append(embedding)
                
                # æ‰“å°è¿›åº¦
                if (i + 1) % 100 == 0:
                    print(f"å·²ç”Ÿæˆ {i + 1}/{len(self.knowledge_units)} ä¸ªçŸ¥è¯†å•å…ƒåµŒå…¥")
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            self.embeddings = np.array(self.embeddings)
            print(f"âœ… å®Œæˆç”ŸæˆçŸ¥è¯†åº“åµŒå…¥: å½¢çŠ¶ {self.embeddings.shape}")
        except Exception as e:
            print(f"âŒ ç”ŸæˆçŸ¥è¯†åº“åµŒå…¥å¤±è´¥: {e}")
            self.embeddings = None
    
    
    def _load_precomputed_embeddings(self):
        """åŠ è½½é¢„è®¡ç®—çš„åµŒå…¥å‘é‡"""
        embedding_cache_dir = "/Users/Williamhiler/Documents/my-project/train/v5/data/expert_knowledge/embeddings"
        embedding_file = os.path.join(embedding_cache_dir, "knowledge_embeddings.npy")
        config_file = os.path.join(embedding_cache_dir, "embedding_config.json")
        
        if os.path.exists(embedding_file) and os.path.exists(config_file):
            print("æ­£åœ¨åŠ è½½é¢„è®¡ç®—çš„åµŒå…¥å‘é‡...")
            try:
                # åŠ è½½åµŒå…¥å‘é‡
                self.embeddings = np.load(embedding_file)
                
                # åŠ è½½é…ç½®
                with open(config_file, 'r', encoding='utf-8') as f:
                    embedding_config = json.load(f)
                
                print(f"âœ… æˆåŠŸåŠ è½½é¢„è®¡ç®—åµŒå…¥: å½¢çŠ¶ {self.embeddings.shape}")
                print(f"   æ¨¡å‹åç§°: {embedding_config.get('model_name')}")
                print(f"   ç”Ÿæˆæ—¶é—´: {embedding_config.get('generated_timestamp')}")
                
                # éªŒè¯åµŒå…¥æ•°é‡ä¸çŸ¥è¯†å•å…ƒæ•°é‡æ˜¯å¦åŒ¹é…
                if len(self.embeddings) == len(self.knowledge_units):
                    return True
                else:
                    print(f"âŒ åµŒå…¥æ•°é‡ä¸åŒ¹é…: {len(self.embeddings)} != {len(self.knowledge_units)}")
                    return False
                    
            except Exception as e:
                print(f"âŒ åŠ è½½é¢„è®¡ç®—åµŒå…¥å¤±è´¥: {e}")
                return False
        
        print("æœªæ‰¾åˆ°é¢„è®¡ç®—çš„åµŒå…¥å‘é‡ï¼Œå°†ç”Ÿæˆæ–°çš„åµŒå…¥")
        return False
    
    
    def _construct_knowledge_text(self, knowledge_unit: Dict) -> str:
        """æ„å»ºçŸ¥è¯†å•å…ƒçš„æ–‡æœ¬è¡¨ç¤º"""
        # ç»“åˆæ ‡é¢˜ã€å†…å®¹ã€çŸ¥è¯†ç±»å‹å’Œå…³é”®æ¦‚å¿µ
        text_parts = [
            knowledge_unit.get("title", ""),
            knowledge_unit.get("content", ""),
            f"çŸ¥è¯†ç±»å‹: {knowledge_unit.get('knowledge_type', '')}",
            f"å…³é”®æ¦‚å¿µ: {', '.join(knowledge_unit.get('key_concepts', []))}"
        ]
        
        return " ".join([part for part in text_parts if part.strip()])
    
    
    def match_relevant_knowledge(self, match_context: Dict, top_k: int = 5) -> List[Dict]:
        """åŒ¹é…ç›¸å…³çš„ä¸“å®¶çŸ¥è¯†"""
        if self.embeddings is None or self.knowledge_units is None:
            return []
        
        # æ„å»ºæ¯”èµ›ä¸Šä¸‹æ–‡çš„æ–‡æœ¬è¡¨ç¤º
        match_text = self._construct_match_text(match_context)
        
        # è°ƒè¯•ï¼šæ‰“å°ç”Ÿæˆçš„ä¸Šä¸‹æ–‡
        print(f"\nè°ƒè¯•ï¼šç”Ÿæˆçš„æ¯”èµ›ä¸Šä¸‹æ–‡")
        print(f"  ä¸Šä¸‹æ–‡é•¿åº¦: {len(match_text)}")
        print(f"  ä¸Šä¸‹æ–‡å†…å®¹: {match_text[:500]}...")
        
        # ç”Ÿæˆæ¯”èµ›ä¸Šä¸‹æ–‡çš„åµŒå…¥
        match_embedding = self.generate_embedding(match_text)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = cosine_similarity([match_embedding], self.embeddings)[0]
        
        # è·å–æœ€ç›¸å…³çš„çŸ¥è¯†å•å…ƒç´¢å¼•
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        # è°ƒè¯•ï¼šæ‰“å°ç›¸ä¼¼åº¦ä¿¡æ¯
        print(f"è°ƒè¯•ï¼šç›¸ä¼¼åº¦ä¿¡æ¯")
        print(f"  æœ€é«˜ç›¸ä¼¼åº¦: {max(similarities):.4f}")
        print(f"  å¹³å‡ç›¸ä¼¼åº¦: {np.mean(similarities):.4f}")
        print(f"  ç›¸ä¼¼åº¦å‰{top_k}: {[f'{similarities[idx]:.4f}' for idx in top_indices]}")
        
        # æ„å»ºç»“æœ
        relevant_knowledge = []
        for idx in top_indices:
            relevance_score = float(similarities[idx])
            if relevance_score > 0.05:  # è¿›ä¸€æ­¥é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œæé«˜åŒ¹é…æˆåŠŸç‡
                relevant_knowledge.append({
                    "index": idx,
                    "unit": self.knowledge_units[idx],
                    "relevance_score": relevance_score
                })
        
        return relevant_knowledge
    
    
    def _construct_match_text(self, match_context: Dict) -> str:
        """æ„å»ºæ¯”èµ›ä¸Šä¸‹æ–‡çš„æ–‡æœ¬è¡¨ç¤º"""
        # ä½¿ç”¨æœ€æ–°çš„ä¸Šä¸‹æ–‡ç”Ÿæˆå™¨ç”Ÿæˆå…¨é¢çš„ä¸Šä¸‹æ–‡ï¼ŒåŒ…å«èµ›æœä¿¡æ¯ç”¨äºè°ƒè¯•
        return self.context_generator.generate_context(match_context, 'knowledge_matching', include_result=True)
    
    
    def get_enhanced_match_result(self, match_context: Dict, top_k: int = 5) -> Dict:
        """è·å–å¢å¼ºçš„åŒ¹é…ç»“æœï¼ŒåŒ…æ‹¬è¯­ä¹‰åˆ†æ"""
        # è·å–ç›¸å…³çŸ¥è¯†
        relevant_knowledge = self.match_relevant_knowledge(match_context, top_k)
        
        # åˆ†æåŒ¹é…ç»“æœ
        knowledge_analysis = self._analyze_match_results(relevant_knowledge)
        
        return {
            "relevant_knowledge": relevant_knowledge,
            "match_analysis": knowledge_analysis,
            "total_matches": len(relevant_knowledge)
        }
    
    
    def _analyze_match_results(self, relevant_knowledge: List[Dict]) -> Dict:
        """åˆ†æåŒ¹é…ç»“æœ"""
        if not relevant_knowledge:
            return {
                "knowledge_type_distribution": {},
                "average_relevance_score": 0.0,
                "highest_relevance_score": 0.0
            }
        
        # ç»Ÿè®¡çŸ¥è¯†ç±»å‹åˆ†å¸ƒ
        type_distribution = {}
        total_relevance = 0.0
        highest_relevance = 0.0
        
        for knowledge in relevant_knowledge:
            knowledge_type = knowledge["unit"]["knowledge_type"]
            relevance_score = knowledge["relevance_score"]
            
            # æ›´æ–°ç±»å‹åˆ†å¸ƒ
            type_distribution[knowledge_type] = type_distribution.get(knowledge_type, 0) + 1
            
            # æ›´æ–°æ€»ç›¸å…³æ€§
            total_relevance += relevance_score
            
            # æ›´æ–°æœ€é«˜ç›¸å…³æ€§
            if relevance_score > highest_relevance:
                highest_relevance = relevance_score
        
        return {
            "knowledge_type_distribution": type_distribution,
            "average_relevance_score": total_relevance / len(relevant_knowledge),
            "highest_relevance_score": highest_relevance
        }


def main():
    """æµ‹è¯•åŸºäºQwençš„çŸ¥è¯†åŒ¹é…å™¨"""
    print("=== åŸºäºQwençš„æ™ºèƒ½çŸ¥è¯†åŒ¹é…å™¨æµ‹è¯• ===")
    
    config = {
        "knowledge_matching": {
            "enabled": True,
            "top_k": 5
        }
    }
    
    try:
        # åˆ›å»ºåŒ¹é…å™¨
        matcher = QwenKnowledgeMatcher(config)
        
        # æµ‹è¯•æ•°æ®
        test_matches = [
            {
                "home_team": "æ›¼åŸ",
                "away_team": "åˆ©ç‰©æµ¦",
                "home_win_odds": 1.8,
                "draw_odds": 3.4,
                "away_win_odds": 4.2
            },
            {
                "home_team": "æ›¼è”",
                "away_team": "åˆ‡å°”è¥¿",
                "home_win_odds": 2.5,
                "draw_odds": 3.2,
                "away_win_odds": 2.8
            }
        ]
        
        for i, match in enumerate(test_matches, 1):
            print(f"\n{'='*60}")
            print(f"æµ‹è¯•æ¯”èµ› {i}: {match['home_team']} vs {match['away_team']}")
            print(f"èµ”ç‡: ä¸»èƒœ{match['home_win_odds']} å¹³å±€{match['draw_odds']} å®¢èƒœ{match['away_win_odds']}")
            
            # è·å–å¢å¼ºçš„åŒ¹é…ç»“æœ
            result = matcher.get_enhanced_match_result(match, top_k=3)
            
            print(f"\nåŒ¹é…ç»“æœ:")
            print(f"- åŒ¹é…åˆ° {result['total_matches']} æ¡ç›¸å…³çŸ¥è¯†")
            print(f"- å¹³å‡ç›¸å…³åº¦: {result['match_analysis']['average_relevance_score']:.3f}")
            print(f"- æœ€é«˜ç›¸å…³åº¦: {result['match_analysis']['highest_relevance_score']:.3f}")
            print(f"- çŸ¥è¯†ç±»å‹åˆ†å¸ƒ: {result['match_analysis']['knowledge_type_distribution']}")
            
            # æ‰“å°åŒ¹é…çš„çŸ¥è¯†
            for j, knowledge in enumerate(result['relevant_knowledge'], 1):
                print(f"\n  åŒ¹é… {j} (ç›¸å…³åº¦: {knowledge['relevance_score']:.3f}):")
                print(f"    æ ‡é¢˜: {knowledge['unit']['title'][:50]}...")
                print(f"    ç±»å‹: {knowledge['unit']['knowledge_type']}")
                print(f"    å†…å®¹: {knowledge['unit']['content'][:100]}...")
        
        print(f"\n{'='*60}")
        print("âœ… åŸºäºQwençš„æ™ºèƒ½çŸ¥è¯†åŒ¹é…å™¨æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    main()