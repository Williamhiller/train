# 模型配置
model:
  name: "meta-llama/Llama-3.2-1B"
  max_length: 1024
  quantization: true  # 是否使用量化（节省内存）
  quant_bits: 4  # 量化位数

# 训练配置
training:
  output_dir: "./models/fine_tuned_model"
  num_epochs: 5
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 8  # 梯度累积，模拟更大的batch size
  learning_rate: 5e-5
  weight_decay: 0.01
  warmup_ratio: 0.05
  logging_steps: 10
  save_steps: 500
  evaluation_strategy: "steps"
  eval_steps: 500
  seed: 42
  fp16: true  # 使用混合精度训练加速
  push_to_hub: false  # 是否推送到Hugging Face Hub
  hub_model_id: "your-hf-username/soccer-prediction-model"  # 如启用push_to_hub，需设置此值

# 数据配置
data:
  train_file: "./data/train.json"
  validation_file: "./data/validation.json"
  test_file: "./data/test.json"
  sample_rate: 1.0  # 数据采样率，可用于调试

# 推理配置
inference:
  model_path: "./models/fine_tuned_model"
  max_new_tokens: 100
  temperature: 0.7
  top_p: 0.9
  do_sample: true

# PEFT (Parameter-Efficient Fine-Tuning) 配置
peft:
  use_peft: true  # 是否使用PEFT方法
  peft_type: "lora"  # 可选: lora, prefix_tuning, p_tuning, prompt_tuning
  lora_r: 16  # LoRA的秩
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# 日志配置
logging:
  log_level: "INFO"
  log_file: "./logs/training.log"